---
title: "Prediction of Crime Category in San Francisco"
author: "Phuc Nguyen, Yun Jae Choi, Katja McKiernan"
date: "December 16, 2016"
output: html_document
---

##Goal:


### Problem Setting:

Our client, the Police Department of San Francisco, wants to identify the type of crimes based on the location, hour, and day of week, so that they can maximize their efficiency in staffing their officers. Specifically, we believe that our method will facilitate the staffing of the officers by summoning specialized officers to designated areas at particular times. Our method will allow for the Police Department to figure out a better way to prevent crimes and to respond quicker to crime incidents. 


### Qualitative Goal:

Our qualitative goal will be to examine if the likelihood (in percentage) derived from using our models is higher than pure chance. Out of five type of crime incidents, the likelihood of one specific crime happening at a random chance would be 20%. Thus, the likelihood that we derive from our method will have to be higher than 20% for it to perform better than that from random chance. 


## The Data:

Our data is about crime incidents in San Francisco from 2003 to 2015. It contains the dates, category of the crime, description of each crime incident, day of the week, name of the Police Department District, resolution, address, and longitude and latitude. We decided to use the  “hour” variable as a simplified marker of the time of the crime. For our explanatory variables, we included hour, the day of the week (weekday or weekend), and the location (in longitude and latitude) to predict the type of crime committed. After running the summary, we found out that the most important predictor variables are the location (latitude and longitude) and the hour that the crime occurred. The response variable is the category of the crimes committed (larceny/theft, assault, drug/narcotic, burglary, robbery). We had no missing data, but we shrank the dataset to a more manageable size by using only the five most-frequently-occurring crimes. 

Following is the link to the dataset: [link](https://www.kaggle.com/c/sf-crime/data)


## Classification Methods Applied:

We applied five different classification methods to our dataset: KNN, LDA/QDA, Tree, and SVM and picked the best model by comparing the test errors from validation set.

```{r}
library(dplyr)
library(rpart)
library(rpart.plot)
library(e1071)
library(ggplot2)
library(MASS)
library(class)
```

```{r}
# Replace raw_name with path to dataset on your computer and run commented-out lines
# raw_name <-  "C:\\Users\\Diana Choi\\Downloads\\train.csv.zip"
# Crime <- readr::read_csv(raw_name, n_max = 5000000)
# save(Crime, file = "Crime.rda")
```

```{r}

load("Crime.rda")
Crime <-
  Crime %>% 
  group_by(Category) %>%
  filter(n() > 20000) %>%
  mutate(hour = lubridate::hour(Dates), month = lubridate::month(Dates)) %>%
  filter(Category %in% c("LARCENY/THEFT", "DRUG/NARCOTIC", "ROBBERY", "BURGLARY", "ASSAULT")) %>%
  mutate(day_num = ifelse(DayOfWeek %in% c("Saturday", "Sunday"), 1,2)) %>%
  ungroup() %>%
  sample_n(size = 10000)

names(Crime)
```

```{r}
samplesize <- floor(0.75 * nrow(Crime))
set.seed(123)
all_indices = c(1:nrow(Crime))
train_indices = sample(seq_len(nrow(Crime)), size = samplesize)
test_indices = setdiff(all_indices, train_indices)
all( ! test_indices %in% train_indices)


# These are DATA FRAMES (DATA)
# Train_data
Train_data = Crime[train_indices, ]
# Test_data
Test_data = Crime[test_indices, ] 
```

### KNN:

For KNN, we tried three different values of k: 1, 50, and 100. k = 50 yielded the lowest test error among all “k”s: 51.7%. The “k” parameter didn’t seem to be the most important in influencing the test error; the test error moved only within one or two percents from k = 1 to k = 100. 

```{r} 
Train_outcome <- Train_data$Category
Test_outcome <- Test_data$Category
Train_matrix = as.matrix(Train_data[ , c("day_num", "X", "Y", "hour")])
Test_matrix =  as.matrix(Test_data[ , c("day_num", "X", "Y","hour")])
```

The test error from KNN models using k = 1, 50, and 100 respectively:

```{r}
knn.pred50 = knn(Train_matrix, as.matrix(Test_matrix), Train_outcome, k = 1)
foo50 <- table(knn.pred50, Test_outcome)
1 - (sum(diag(foo50))/ sum(foo50))

knn.pred50 = knn(Train_matrix, as.matrix(Test_matrix), Train_outcome, k = 50)
foo50 <- table(knn.pred50, Test_outcome)
1 - (sum(diag(foo50))/ sum(foo50))

knn.pred50 = knn(Train_matrix, as.matrix(Test_matrix), Train_outcome, k = 100)
foo50 <- table(knn.pred50, Test_outcome)
1 - (sum(diag(foo50))/ sum(foo50))
```

We used cross validation to find error rate of our model. (We've commented out the code because it takes a long time to run):

```{r}
# kntune <- tune.knn(Train_matrix, as.factor(Train_outcome), k=c(1,30,50,70,100), tunecontrol=tune.control(sampling = "boot"))
# summary(kntune)
```
Parameter tuning of ‘knn.wrapper’:

- sampling method: bootstrapping 

- best parameters:
  k
 50

- best performance: 0.5171235 

- Detailed performance results:

 |  k  |    error |  dispersion
 |-----|----------|------------
1|   1 | 0.6011005| 0.006954314
2|  30 | 0.5195495| 0.005045945
3|  50 | 0.5171235| 0.006644788
4|  70 | 0.5246869| 0.004692278
5| 100 | 0.5182566| 0.007749336

The model with k=50 produced the smallest test error of 51.7%


### LDA and QDA

We also tried LDA and QDA, which yielded similar test errors to that of KNN model. There were no meta-parameters that we had to set. 

```{r}
set.seed(7)
#Linear Discriminant Analysis

mod_lda <- lda(Category~ day_num + X + Y + hour + month, data=Train_data)
pred_lda <- predict(mod_lda, newdata=Test_data)
foo <- table(Test_data$Category, pred_lda$class)
1 - (sum(diag(foo))/sum(foo))

#Quadratic Discriminant Analysis, more flexible than LDA

mod_qda <- qda(Category~ day_num + X + Y + hour + month, data=Train_data)
pred_qda <- predict(mod_qda, newdata=Test_data)
foo <- table(Test_data$Category, pred_qda$class)
1- (sum(diag(foo))/sum(foo))
```

We believe that the differences in the test errors across KNN, LDA and QDA are due to flexibility. KNN is an a lot more flexible method than LDA or QDA since it is a nonparametric method. The randomness of crimes in terms of location and time may explain why more flexible methods such as KNN performs better than less flexible methods such as LDA and QDA.


### SVM:

For SVM, the lowest test error was 50.7% for “radial” type. Again, the more flexible model parameter "radial" as opposed to "linear" performed better. We used the function tune() to find the optimal parameters: cost = 100 and gamma = 1 (We've commented out the code to optimize these parameters because it takes a long time to run). However, the optimal parameters improved prediction accuracy by only one percent from our original guess.

```{r}
# Radial model
mod <- svm(as.factor(Category) ~ day_num + X + Y + hour, data=Train_data, kernel = "radial", cost = 10, gamma = 1)
pred <- predict(mod, newdata=Test_data)
foo <- table(Test_data$Category, pred)
1- (sum(diag(foo))/sum(foo))

# Linear model
mod <- svm(as.factor(Category) ~ day_num + X + Y + hour, data=Train_data, kernel = "linear", cost = 10)
pred <- predict(mod, newdata=Test_data)
foo <- table(Test_data$Category, pred)
1- (sum(diag(foo))/sum(foo))
```

The "radial" model performed better than the "linear" model.

We used cross validation to estimate error rate and choose the best values for cost and gamma:

```{r}
# Tune model
# svmtune <- tune(svm, as.factor(Category)~day_num+X+Y+hour, data=Train_data, kernel="radial", ranges=list(cost=c(1,10,100), gamma=c(0.5,1,2)), tunecontrol = tune.control(sampling = "fix", cross=5))

# Best parameters: cost=100, gamma=1
```

Parameter tuning of ‘svm’:

- sampling method: fixed training/validation set

- best parameters:
 cost gamma
  100     1
  
- best performance: 0.5076 

- Detailed performance results:

  |cost | gamma | error  
  |-----|-------|-------
1 |  1  | 0.5   | 0.5168      
2 |  10 |  0.5  | 0.5164      
3 | 100 |  0.5  | 0.5116       
4 |   1 |  1.0  | 0.5160      
5 |  10 |  1.0  | 0.5132     
6 | 100 |  1.0  | 0.5076        
7 |   1 |  2.0  | 0.5140       
8 |  10 |  2.0  | 0.5076        
9 | 100 |  2.0  | 0.5120         

The optimized svm model using cost=100, gamma=1 returned an error rate of 50.7%.

### Tree:

The Tree method using all five predictors returned 47.2% of test error. We first found the best-performing tuning parameters, cp = 0.001 and maxdepth = 20, and got a 10-fold cross-validated test error of 47.2%, which is the lowest among all five models.

We used cross validation to find best values for parameters minsplit, cp, and maxdepth (some code is commented out because it takes a long time to run):

```{r}
# Tune min-split, cp and max-depth:
fm <- formula(as.factor(Category)~X+Y+month+day_num+hour)

t.tree <- tune.rpart(fm, data=Train_data, cp = c(0.0001,0.0005,0.001, 0.005, 0.01))
plot(t.tree, main="cp" )

t.tree.minsplit <- tune.rpart(fm, data=Train_data, minsplit = c(10, 20, 50,100))
plot(t.tree.minsplit, main="minsplit")

# t.tree.maxdepth <- tune.rpart(fm, data=Train_data, maxdepth =  c(5,20,25,30), cp=0.001)
# summary(t.tree.maxdepth)

# maxdepth = 20 and cp = 0.001 shows the best performance

tree.Train_Everything <- rpart(Category~X+Y+month+day_num+hour, data = Train_data, control = rpart.control(cp=0.001, maxdepth = 20))
rpart.plot::prp(tree.Train_Everything)
```

Parameter tuning of ‘rpart.wrapper’:

- sampling method: 10-fold cross validation 

- best parameters:
    cp maxdepth
 0.001       20

- best performance: 0.4726667 

- Detailed performance results:

 |   cp|maxdepth|  error  |dispersion
 |-----|--------|---------|-------------
1|0.001|    5   |0.4808000| 0.008959718
2|0.001|   20   |0.4726667| 0.009412364
3|0.001|   25   |0.4726667| 0.009412364
4|0.001|   30   |0.4726667| 0.009412364

Minsplit didn't affect the model much. The optimal values for maxdepth (maximum depth of the tree) was 20 and cp (complexity parameter) was 0.001. 

## Evaluation of Methods:

The qualitative goal of our project is to predict the type of crime with an accuracy rate of higher than 20%, so that the police department can send specialized officers more quickly and efficiently when a call is received. Our best model produced the accuracy rate of 52.8% (Tree). Hence, specialized officers are more likely to arrive at the crime scenes quicker and have more appropriate action plans to reduce injury or material loss for all parties involved. While our method does not guarantee correct identifications, the fact that it renders higher accuracy rates of correctly identifying the crime at a certain location and time compared to that at random chance is still valuable.


## Alternative Approaches:

```{r}
set.seed(6)
in_pred <- predict(tree.Train_Everything, newdata=Train_data, type="class")
Train_data$in_pred <- in_pred

ggplot(Train_data,aes(X,Y, colour=in_pred))  + geom_point(aes(alpha=1/3)) + xlim(-122.35, -122.55) + ylim(37.7,37.85) + labs(title ="Tree Predicted Crime Category")

ggplot(Train_data, aes(X, Y, colour = Category)) + geom_point(alpha=1/3) + xlim(-122.35, -122.55) + ylim(37.7,37.85) + labs(title ="Actual Crime Category")
```


As one can see from the graph above, some crimes, such as Assault, tend to happen more in the southern part of the city, while Larceny is more likely to happen in the northwest region. In addition, our Tree model created quite distinct geographic bounderies for different crime categories. We think we can improve on this model by using SVM on different branches of the tree. This alternative method may allow for more Larceny cases in the southern part of the city.

On a similar note, we believe that we can improve our results by segmenting the cities into bigger geographic regions with real estate values and optimizing separate models for each regions. Along the same line, we can group other similar variables into larger categories to improve test error. For example, bulglary and theft/larceny can be lumped together because both involve appropriations of someone else's property.
